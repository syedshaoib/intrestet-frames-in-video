{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85ca1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ce4015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793a061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c48af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4558 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   validation_split=0.6,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9f5f6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1557 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "test_set = train_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ed9edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = tf.keras.models.Sequential()\n",
    "# resnet=tf.keras.applications.ResNet50(include_top=False,input_shape=(244,244,3),weights='imagenet')\n",
    "# for layer in resnet.layers:\n",
    "#     layer.trainable=False\n",
    "# cnn.add(resnet)\n",
    "# cnn.add(tf.keras.layers.AveragePooling2D(pool_size=(7,7)))\n",
    "\n",
    "# cnn.add(tf.keras.layers.Flatten())\n",
    "# cnn.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "# cnn.add(tf.keras.layers.Dropout(0.5))\n",
    "# cnn.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "# cnn.add(tf.keras.layers.Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f6f58dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58c252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc4ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b44e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "143/143 [==============================] - 487s 3s/step - loss: 0.3453 - accuracy: 0.8741 - val_loss: 0.6261 - val_accuracy: 0.6410\n",
      "Epoch 2/25\n",
      "143/143 [==============================] - 65s 450ms/step - loss: 0.2841 - accuracy: 0.8850 - val_loss: 0.4698 - val_accuracy: 0.7636\n",
      "Epoch 3/25\n",
      "143/143 [==============================] - 94s 660ms/step - loss: 0.2461 - accuracy: 0.9024 - val_loss: 0.5070 - val_accuracy: 0.7508\n",
      "Epoch 4/25\n",
      "143/143 [==============================] - 132s 923ms/step - loss: 0.2350 - accuracy: 0.9100 - val_loss: 0.5284 - val_accuracy: 0.7296\n",
      "Epoch 5/25\n",
      "143/143 [==============================] - 77s 543ms/step - loss: 0.2278 - accuracy: 0.9096 - val_loss: 0.3432 - val_accuracy: 0.8638\n",
      "Epoch 6/25\n",
      "143/143 [==============================] - 102s 713ms/step - loss: 0.2050 - accuracy: 0.9175 - val_loss: 0.3413 - val_accuracy: 0.8497\n",
      "Epoch 7/25\n",
      "143/143 [==============================] - 114s 798ms/step - loss: 0.1910 - accuracy: 0.9265 - val_loss: 0.4594 - val_accuracy: 0.7720\n",
      "Epoch 8/25\n",
      "143/143 [==============================] - 88s 616ms/step - loss: 0.1912 - accuracy: 0.9243 - val_loss: 0.2950 - val_accuracy: 0.8793\n",
      "Epoch 9/25\n",
      "143/143 [==============================] - 125s 878ms/step - loss: 0.1799 - accuracy: 0.9305 - val_loss: 0.2985 - val_accuracy: 0.8658\n",
      "Epoch 10/25\n",
      "143/143 [==============================] - 80s 561ms/step - loss: 0.1645 - accuracy: 0.9331 - val_loss: 0.3126 - val_accuracy: 0.8645\n",
      "Epoch 11/25\n",
      "143/143 [==============================] - 80s 558ms/step - loss: 0.1602 - accuracy: 0.9381 - val_loss: 0.2893 - val_accuracy: 0.8741\n",
      "Epoch 12/25\n",
      "143/143 [==============================] - 118s 832ms/step - loss: 0.1560 - accuracy: 0.9377 - val_loss: 0.2228 - val_accuracy: 0.8998\n",
      "Epoch 13/25\n",
      "143/143 [==============================] - 79s 547ms/step - loss: 0.1444 - accuracy: 0.9421 - val_loss: 0.2824 - val_accuracy: 0.8709\n",
      "Epoch 14/25\n",
      "143/143 [==============================] - 131s 923ms/step - loss: 0.1258 - accuracy: 0.9500 - val_loss: 0.2157 - val_accuracy: 0.9062\n",
      "Epoch 15/25\n",
      "143/143 [==============================] - 192s 1s/step - loss: 0.1284 - accuracy: 0.9498 - val_loss: 0.2499 - val_accuracy: 0.8863\n",
      "Epoch 16/25\n",
      "143/143 [==============================] - 116s 805ms/step - loss: 0.1177 - accuracy: 0.9537 - val_loss: 0.1771 - val_accuracy: 0.9326\n",
      "Epoch 17/25\n",
      "143/143 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9548"
     ]
    }
   ],
   "source": [
    "cnn.fit(training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222cb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('olam/test/swimming/00000388.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "label=(np.argmax(result[0]))\n",
    "print(result,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ae9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.array(result[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j=-1\n",
    "list=[]\n",
    "list2=[]\n",
    "def get(frame,vid,fps):\n",
    "    frames = vid.get(cv2.CAP_PROP_FRAME_COUNT) \n",
    "\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS)) \n",
    "\n",
    "    frame=cv2.resize(frame,(64,64))\n",
    "    test_image = np.expand_dims(frame, axis = 0)\n",
    "    result = cnn.predict(test_image)\n",
    "  \n",
    "    return (np.argmax(result[0]))\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    " \n",
    " \n",
    "\n",
    "       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6542316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc69eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15ca00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    " \n",
    " \n",
    "\n",
    " \n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    " \n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    "vid=cv2.VideoCapture('swim.mp4')\n",
    "import datetime \n",
    "\n",
    "\n",
    "dog=[]\n",
    "male=[]\n",
    "fps = int(vid.get(cv2.CAP_PROP_FPS)) \n",
    "count=0\n",
    "ret=True\n",
    "while ret :\n",
    "    ret,image=vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "  \n",
    "#     fps = int(vid.get(cv2.CAP_PROP_FPS)) \n",
    "#     print(fps)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ret,frame=vid.read()\n",
    "    count+=1\n",
    "    \n",
    "   \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    new_frame_time = time.time()\n",
    "#     print((vid.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    " \n",
    " \n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    " \n",
    " \n",
    "    # converting the fps into integer\n",
    "   \n",
    "   \n",
    "    # converting the fps to string so that we can display it on frame\n",
    "    # by using putText function\n",
    "\n",
    "    label=get(image,vid,fps)\n",
    "    if(label==0):\n",
    "        dog.append(('dog',count/fps))\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        male.append(('male',count/fps))\n",
    " \n",
    "    \n",
    " \n",
    "    # putting the FPS count on the frame\n",
    "#     cv2.putText(image, fps, (7, 70), font, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "    x.append(image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915deeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dog apperaed in video between '+ str( dog[0][1])+\"  \"+str(dog[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('male apperaed in video between '+ str( male[0][1])+\"  \"+str(male[-1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bee291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760d6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf901a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb1ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e6c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
